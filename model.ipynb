{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Loading DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = np.genfromtxt('Data\\magic04.data', delimiter=',', dtype=str) # Data is in the form of array of tuples\n",
    "labels = data_set[:, len(data_set[0]) - 1:len(data_set[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "under_sampler = RandomUnderSampler()\n",
    "sampled_data, sampled_labels = under_sampler.fit_resample(data_set, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "unique, counts = np.unique(sampled_labels, return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_set, testing_data_set = train_test_split(data_set, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.array(training_data_set[:, 0:len(training_data_set[0]) - 1]).astype(np.float64)\n",
    "training_labels = training_data_set[:, len(training_data_set[0]) - 1:len(training_data_set[0])]\n",
    "training_labels = np.reshape(training_labels, len(training_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = np.array(testing_data_set[:, 0:len(testing_data_set[0]) - 1]).astype(np.float64)\n",
    "testing_labels = testing_data_set[:, len(testing_data_set[0]) - 1:len(testing_data_set[0])]\n",
    "testing_labels = np.reshape(testing_labels, len(testing_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(tr_data, tr_labels, tst_data):\n",
    "    decision_tree = tree.DecisionTreeClassifier()\n",
    "    decision_tree_pred = decision_tree.fit(tr_data, tr_labels).predict(tst_data)\n",
    "    return decision_tree_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ada_boost(tr_data, tr_labels, tst_data, n_estimators):\n",
    "    ada_boost = AdaBoostClassifier(n_estimators=n_estimators)\n",
    "    ada_boost_pred = ada_boost.fit(tr_data, tr_labels).predict(tst_data)\n",
    "    return ada_boost_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) K-Nearest Neighbors (K-NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(tr_data, tr_labels, tst_data, k_neighb = 3):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k_neighb)\n",
    "    knn_pred = neigh.fit(tr_data, tr_labels).predict(tst_data)\n",
    "    return knn_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Random Forests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forests(tr_data, tr_labels, tst_data, n_estimators):\n",
    "    random_forests = RandomForestClassifier(n_estimators=n_estimators)\n",
    "    random_forests_pred = random_forests.fit(tr_data, tr_labels).predict(tst_data)\n",
    "    return random_forests_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Na¨ıve Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(tr_data, tr_labels, tst_data):\n",
    "    gnb = GaussianNB()\n",
    "    naive_bayes_pred = gnb.fit(tr_data, tr_labels).predict(tst_data)\n",
    "    return naive_bayes_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Parameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_pred = decision_tree(training_data, training_labels, testing_data)\n",
    "print(\"Decision tree accuracy: \", accuracy_score(testing_labels, decision_tree_pred) * 100, \"%\")\n",
    "precision_score(testing_labels, decision_tree_pred, average='weighted')\n",
    "recall_score(testing_labels, decision_tree_pred, average='weighted')\n",
    "f1_score(testing_labels, decision_tree_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes_pred = naive_bayes(training_data, training_labels, testing_data)\n",
    "print(\"Na¨ıve bayes accuracy: \", accuracy_score(testing_labels, naive_bayes_pred) * 100, \"%\")\n",
    "precision_score(testing_labels, naive_bayes_pred, average='weighted')\n",
    "recall_score(testing_labels, naive_bayes_pred, average='weighted')\n",
    "f1_score(testing_labels, naive_bayes_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_neighb = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "for k in k_neighb:\n",
    "    knn_pred = knn(training_data, training_labels, testing_data, k)\n",
    "    print(str(k) + \"-NN accuracy: \", accuracy_score(testing_labels, knn_pred) * 100, \"%\")\n",
    "    precision_score(testing_labels, knn_pred, average='weighted')\n",
    "    recall_score(testing_labels, knn_pred, average='weighted')\n",
    "    f1_score(testing_labels, knn_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n",
    "for n in n_estimators:\n",
    "    ada_boost_pred = ada_boost(training_data, training_labels, testing_data, n)\n",
    "    print(str(n) + \"_estimators AdaBoost accuracy: \", accuracy_score(testing_labels, ada_boost_pred) * 100, \"%\")\n",
    "    precision_score(testing_labels, ada_boost_pred, average='weighted')\n",
    "    recall_score(testing_labels, ada_boost_pred, average='weighted')\n",
    "    f1_score(testing_labels, ada_boost_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n",
    "for n in n_estimators:\n",
    "    random_forests_pred = random_forests(training_data, training_labels, testing_data, n)\n",
    "    print(str(n) + \"_estimators random forests accuracy: \", accuracy_score(testing_labels, random_forests_pred) * 100, \"%\")\n",
    "    precision_score(testing_labels, random_forests_pred, average='weighted')\n",
    "    recall_score(testing_labels, random_forests_pred, average='weighted')\n",
    "    f1_score(testing_labels, random_forests_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Report Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "training_encoded_labels = le.fit_transform(training_labels)\n",
    "testing_encoded_labels = le.fit_transform(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training Data Shape {training_data.shape}')\n",
    "print(f'Training Label Shape {training_encoded_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training Data  {training_data[0]}')\n",
    "print(f'Training Label {training_encoded_labels[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val = torch.from_numpy(training_data).float()\n",
    "X_test = torch.from_numpy(testing_data).float()\n",
    "y_train_val = torch.from_numpy(training_encoded_labels).float()\n",
    "y_test = torch.from_numpy(testing_encoded_labels).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, input_layer, hidden_layer_1, hidden_layer_2, output_layer):\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.l1 = nn.Linear(input_layer, hidden_layer_1)\n",
    "        self.l2 = nn.Linear(hidden_layer_1, hidden_layer_2)\n",
    "        self.out = nn.Linear(hidden_layer_2, output_layer)\n",
    " \n",
    "    def forward(self, data):\n",
    "        x = self.l1(data)\n",
    "        x = torch.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.out(x)\n",
    "        return torch.sigmoid(x)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_val_model(model, epochs, optimizer, loss_fn, X_train, X_val, y_train, y_val):\n",
    "    acc_data_val = []\n",
    "    acc_data_train = []\n",
    "    loss_data_val = []\n",
    "    loss_data_train = []\n",
    "\n",
    "    for epoch in range(EPOCHS+1):\n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "        y_pred_train = model(X_train)\n",
    "        loss_train = loss_fn(y_pred_train, y_train)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        y_pred_val = model(X_val)\n",
    "        loss_val = loss_fn(y_pred_val, y_val)\n",
    "        condition = y_pred_val >= 0.5\n",
    "        y_pred_val = torch.where(condition, 1, 0)\n",
    "        val_acc = torch.sum(y_pred_val == y_val) / len(y_val)\n",
    "\n",
    "        condition = y_pred_train >= 0.5\n",
    "        y_pred_train = torch.where(condition, 1, 0)\n",
    "        train_acc = torch.sum(y_pred_train == y_train) / len(y_train)\n",
    "        if epoch % 250 == 0:\n",
    "            print(f'{epoch} : Training loss {loss_train} && Training Acc is {train_acc} || Test loss {loss_val} && Test Acc is {val_acc}')\n",
    "        acc_data_val.append(val_acc)\n",
    "        acc_data_train.append(train_acc)\n",
    "        loss_data_val.append(loss_val.detach().numpy())\n",
    "        loss_data_train.append(loss_train.detach().numpy())\n",
    "\n",
    "    return acc_data_train, acc_data_val, loss_data_train, loss_data_val\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = ClassificationModel(10, 16, 32, 1)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_1.parameters(), lr=1e-4)\n",
    "acc_data_train_1, acc_data_val_1, loss_data_train_1, loss_data_val_1 = train_val_model(model_1, 2000, optimizer, loss_fn, X_train, X_val, y_train.reshape(-1, 1), y_val.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = ClassificationModel(10, 64, 128, 1)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_2.parameters(), lr=1e-4)\n",
    "acc_data_train_2, acc_data_val_2, loss_data_train_2, loss_data_val_2 = train_val_model(model_2, 2000, optimizer, loss_fn, X_train, X_val, y_train.reshape(-1, 1), y_val.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = ClassificationModel(10, 128, 512, 1)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model_3.parameters(), lr=1e-4)\n",
    "acc_data_train_3, acc_data_val_3, loss_data_train_3, loss_data_val_3 = train_val_model(model_3, 2000, optimizer, loss_fn, X_train, X_val, y_train.reshape(-1, 1), y_val.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_loss(acc_data_train, acc_data_val, loss_data_train, loss_data_val):\n",
    "    x_axis = range(0, EPOCHS+1)\n",
    "    fig, axs = plt.subplots(1, 2, figsize=[18, 5])\n",
    "    \n",
    "    axs[0].plot(x_axis, acc_data_train, '--', color=\"r\",  label=\"Training score\")\n",
    "    axs[0].plot(x_axis, acc_data_val, color=\"b\", label=\"Cross-validation score\")\n",
    "    axs[0].set_title(\"Accuracy Curve\")\n",
    "    axs[0].set_xlabel(\"Training Set Size\")\n",
    "    axs[0].set_ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "    axs[0].legend()\n",
    "    \n",
    "    axs[1].plot(x_axis, loss_data_train, '--', color=\"r\",  label=\"Training loss\")\n",
    "    axs[1].plot(x_axis, loss_data_val, color=\"b\", label=\"Cross-validation loss\")\n",
    "    axs[1].set_title(\"Loss Curve\")\n",
    "    axs[1].set_xlabel(\"Training Set Size\")\n",
    "    axs[1].set_ylabel(\"Loss Score\"), plt.legend(loc=\"best\")\n",
    "    axs[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss(acc_data_train_1, acc_data_val_1, loss_data_train_1, loss_data_val_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss(acc_data_train_2, acc_data_val_2, loss_data_train_2, loss_data_val_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss(acc_data_train_3, acc_data_val_3, loss_data_train_3, loss_data_val_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute_test(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    y_pred_test = model(X_test)\n",
    "    cond = y_pred_test >= 0.5\n",
    "    y_pred_test = torch.where(cond, 1, 0)\n",
    "    train_acc = (torch.sum(y_pred_test == y_test) / len(y_test)).round(4)\n",
    "    print(f\"Model Acc. on Test Data {train_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalute_test(model_1, X_test, y_test)\n",
    "evalute_test(model_2, X_test, y_test)\n",
    "evalute_test(model_3, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c20b7c458658bee694630567ba3e0a0589b3365f748196e154460478b4d51f10"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
